{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from constants import BLOCK_SIZE, BATCH_SIZE, VOCAB_SIZE, N_EMBED, DEVICE, BIAS, N_HEADS, N_LAYERS, DROP_OUT, WANDB_LOG\n",
    "from model import BigramLanguageModel\n",
    "from train import get_batch\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "from model import GPT\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "decode = lambda x: enc.decode(x)\n",
    "from contextlib import nullcontext\n",
    "from transformers import GPT2LMHeadModel\n",
    "print(f\"Model setup:\\n -------------\\n {VOCAB_SIZE=}\\n {BLOCK_SIZE=}\\n {BATCH_SIZE=}\\n {N_EMBED=}\\n {BIAS=}\\n {N_HEADS=}\\n {N_LAYERS=}\")\n",
    "print(f'--------------')\n",
    "print(f\"Training setup:\")\n",
    "print(f'--------------')\n",
    "print(f\" {DEVICE=}\\n {WANDB_LOG=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the Bi-gram Language Model\n",
    "model = BigramLanguageModel(VOCAB_SIZE)\n",
    "data, targets = get_batch('eval')\n",
    "logits = model(data)\n",
    "B, T, C = logits.shape    # B = BATCH, T = sequence, C = embedding\n",
    "logits = logits.view(B*T, C)\n",
    "print(f\"{B=}, {T=}, {C=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = 1\n",
    "out = model.predict_next(data ,num_predictions)\n",
    "for i in range(data.shape[0]): \n",
    "    input = decode(data[i].tolist())\n",
    "    target = decode(targets[i, -num_predictions:].tolist())\n",
    "    pred = decode(out[i, -num_predictions:].tolist())\n",
    "    print(f\"{target=},\\t\\t\\t {pred=} \\t\\t {input=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "train(model, optimizer, num_epochs=10, run_name='bigram-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementing GPT\n",
    "config = {\"n_heads\": N_HEADS, \n",
    "          \"n_embed\": N_EMBED,\n",
    "          \"block_size\": BLOCK_SIZE,\n",
    "          \"n_layers\": N_LAYERS, \n",
    "          \"bias\": BIAS, \n",
    "          'dropout': DROP_OUT, \n",
    "          'vocab_size': VOCAB_SIZE}\n",
    "config\n",
    "mogpt = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the model without training\n",
    "data, targets = get_batch('eval', mogpt.get_config())\n",
    "num_predictions = 5\n",
    "y = mogpt.predict_next(data, num_predictions)\n",
    "for i in range(len(y)): \n",
    "   print(f\" targets: \\t\\t {decode(targets[i,-num_predictions:].tolist())}, \\n prediction: \\t\\t {decode(y[i,-num_predictions:].tolist())} \\n *-----------------*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model\n",
    "from train import train\n",
    "optimizer = torch.optim.Adam(mogpt.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "train(mogpt, optimizer, num_epochs=10, run_name='gpt-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = 5\n",
    "y = mogpt.predict_next(data, num_predictions)\n",
    "for i in range(len(y)): \n",
    "   print(f\" targets: \\t\\t {decode(targets[i,-num_predictions:].tolist())}, \\n prediction: \\t\\t {decode(y[i,-num_predictions:].tolist())} \\n *-----------------*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading gpt-2 from hugging face\n",
    "pretrained_mogpt= mogpt.load_pretrained_model('gpt2-large')\n",
    "data, targets = get_batch('eval', pretrained_mogpt.get_config())\n",
    "num_predictions = 5\n",
    "y = pretrained_mogpt.predict_next(data, num_predictions)\n",
    "for i in range(len(y)): \n",
    "   print(f\" targets: \\t\\t {decode(targets[i,-num_predictions:].tolist())}, \\n prediction: \\t\\t {decode(y[i,-num_predictions:].tolist())} \\n *-----------------*\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
